{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54cd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e7a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ACCELERATE_CONFIG_FILE\"] = \"/home/eoeldroal/WorkPlace/Culture/accelerate_ds_zero3.yaml\"  # ← 너의 YAML 절대경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a4e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"NCSOFT/VARCO-VISION-2.0-14B\"\n",
    "DATASET = \"HuggingFaceH4/llava-instruct-mix-vsft\"\n",
    "OUTPUT_DIR = \"runs/varco-14b-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56d73f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad78766a08647e3bb19dedbf68521dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d005f49e8e4509a22a073558755ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625d0c2a0b914c2ab06b10f18967a9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################\n",
    "# Dataset\n",
    "################\n",
    "dataset = load_dataset(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98b1877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a02f65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_message_format(messages):\n",
    "    \"\"\"메시지 형식을 VLM 표준으로 변환\"\"\"\n",
    "    converted = []\n",
    "    for message in messages:\n",
    "        converted_message = {\n",
    "            \"role\": message[\"role\"],\n",
    "            \"content\": []\n",
    "        }\n",
    "        for content in message[\"content\"]:\n",
    "            if content[\"type\"] == \"text\" and content[\"text\"]:\n",
    "                converted_message[\"content\"].append({\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": content[\"text\"].strip()\n",
    "                })\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                converted_message[\"content\"].append({\"type\": \"image\"})\n",
    "        \n",
    "        if converted_message[\"content\"]:\n",
    "            converted.append(converted_message)\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b43eb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410a87b23d714d1db84107223332ca6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting message format:   0%|          | 0/259155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834970cc518f4a2da2c0e6e74e0fb245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting message format:   0%|          | 0/13640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_messages(example):\n",
    "    example[\"messages\"] = convert_message_format(example[\"messages\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess_messages, desc=\"Converting message format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d10652f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 기존 collate_fn 그대로 사용\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    images = [example[\"images\"][0] for example in examples]\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(images=images, text=texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7b1a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 후 첫 번째 메시지:\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': 'Who wrote this book?'}, {'type': 'image'}]}\n",
      "✅ 템플릿 적용 성공!\n"
     ]
    }
   ],
   "source": [
    "# 메시지 변환 테스트\n",
    "sample = dataset[\"train\"][0]\n",
    "converted = convert_message_format(sample[\"messages\"])\n",
    "\n",
    "print(\"변환 후 첫 번째 메시지:\")\n",
    "print(converted[0])\n",
    "\n",
    "# apply_chat_template 테스트\n",
    "try:\n",
    "    text = processor.apply_chat_template(converted, tokenize=False)\n",
    "    print(\"✅ 템플릿 적용 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 아직 문제 있음: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6560dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    bf16=True, fp16=False,\n",
    "    learning_rate=6e-6, weight_decay=0.1,\n",
    "    chat_template_path=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    max_steps = 16,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    \n",
    "    max_length=32768,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    ddp_find_unused_parameters=False,\n",
    "\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2836ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split = \"validation\" if \"validation\" in collated_data else (\"test\" if \"test\" in collated_data else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main() :\n",
    "    ################\n",
    "    # Model, Tokenizer & Processor\n",
    "    ################\n",
    "\n",
    "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=None,                # ← DeepSpeed/Accelerate가 배치\n",
    "    )\n",
    "    model.config.use_cache = False     # grad_ckpt와 충돌 방지\n",
    "    # tok = processor.tokenizer\n",
    "    # # (1) pad/eos 보장 (LLaMA/Qwen 계열은 pad 없는 경우 흔함)\n",
    "    # if tok.pad_token is None:\n",
    "    #     tok.pad_token = tok.eos_token\n",
    "    #     tok.pad_token_id = tok.eos_token_id\n",
    "    # tok.padding_side = \"right\"\n",
    "\n",
    "    # # (2) TRL 호환을 위해 Processor에 토큰 관련 속성/메서드 \"위임\"\n",
    "    # #     - SFTTrainer가 processing_class.convert_tokens_to_ids(...)를 호출하므로, 토크나이저 메서드로 연결\n",
    "\n",
    "    # #! 아래가 바로 병목점이다.\n",
    "    # #! qwen 3 에 없는 토큰들을 SFT 단계에서 요구함.\n",
    "    # if not hasattr(processor, \"convert_tokens_to_ids\"):\n",
    "    #     processor.convert_tokens_to_ids = tok.convert_tokens_to_ids\n",
    "    # processor.pad_token = tok.pad_token\n",
    "    # processor.eos_token = tok.eos_token\n",
    "    # processor.pad_token_id = tok.pad_token_id           # ← 추가\n",
    "    # processor.eos_token_id = tok.eos_token_id           # ← 추가\n",
    "\n",
    "\n",
    "    # # (3) 모델 config에도 pad_token_id 반영\n",
    "    # if getattr(model.config, \"pad_token_id\", None) is None:\n",
    "    #     model.config.pad_token_id = tok.pad_token_id\n",
    "    \n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=collate_fn,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=(dataset[eval_split] if eval_split else None),\n",
    "        processing_class=processor,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd33242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 CUDAs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "W0820 22:13:46.062000 146922 site-packages/torch/multiprocessing/spawn.py:169] Terminating process 151978 via signal SIGTERM\n",
      "W0820 22:13:46.066000 146922 site-packages/torch/multiprocessing/spawn.py:169] Terminating process 151979 via signal SIGTERM\n",
      "W0820 22:13:46.068000 146922 site-packages/torch/multiprocessing/spawn.py:169] Terminating process 151980 via signal SIGTERM\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] failed (exitcode: 1) local_rank: 3 (pid: 151981) of fn: train_main (start_method: fork)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] Traceback (most recent call last):\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 692, in _poll\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     self._pc.join(-1)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 215, in join\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] \n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] -- Process 3 terminated with the following error:\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] Traceback (most recent call last):\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     fn(i, *args)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 616, in _wrap\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     ret = record(fn)(*args_)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     return f(*args, **kwargs)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/tmp/ipykernel_146922/754687456.py\", line 40, in train_main\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     train_dataset=dataset[\"train\"],\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2858, in __getitem__\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     return Column(self, key)\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]   File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 656, in __init__\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737]     raise ValueError(f\"Column '{column_name}' doesn't exist.\")\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] ValueError: Column 'train' doesn't exist.\n",
      "E0820 22:13:47.147000 146922 site-packages/torch/distributed/elastic/multiprocessing/api.py:737] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\ntrain_main FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-08-20_22:13:43\n  host      : sv3-a100.tail184f17.ts.net\n  rank      : 3 (local_rank: 3)\n  exitcode  : 1 (pid: 151981)\n  error_file: /tmp/torchelastic_q8z7okcu/none_f6yz99xe/attempt_0/3/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_146922/754687456.py\", line 40, in train_main\n      train_dataset=dataset[\"train\"],\n    File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2858, in __getitem__\n      return Column(self, key)\n    File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 656, in __init__\n      raise ValueError(f\"Column '{column_name}' doesn't exist.\")\n  ValueError: Column 'train' doesn't exist.\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Culture/lib/python3.10/site-packages/accelerate/launchers.py:247\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    246\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 247\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/launcher/api.py:139\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/launcher/api.py:270\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    271\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    272\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\ntrain_main FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-08-20_22:13:43\n  host      : sv3-a100.tail184f17.ts.net\n  rank      : 3 (local_rank: 3)\n  exitcode  : 1 (pid: 151981)\n  error_file: /tmp/torchelastic_q8z7okcu/none_f6yz99xe/attempt_0/3/error.json\n  traceback : Traceback (most recent call last):\n    File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_146922/754687456.py\", line 40, in train_main\n      train_dataset=dataset[\"train\"],\n    File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2858, in __getitem__\n      return Column(self, key)\n    File \"/home/eoeldroal/miniconda3/envs/Culture/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 656, in __init__\n      raise ValueError(f\"Column '{column_name}' doesn't exist.\")\n  ValueError: Column 'train' doesn't exist.\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(train_main, num_processes=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Culture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
